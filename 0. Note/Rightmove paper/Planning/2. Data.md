---
tags:
  - note
  - uni/paper
  - uni/planning
Project:
  - "[[Rightmove paper]]"
Status: Finished
---
\section{Data}
\subsection{Sources and variables}

\textnormal{Our data comes from property data is sourced from a daily scrape of online property listings. The data is localised to the United Kingdom (UK) covering a large number of the UK's online property listings in both the rental and sale markets. 
The property listing data includes a large assortment of characteristics. Location characteristics such as, street address, postcode, exact/approximate latitude and longitude. Layout characteristics including, number of bedrooms, bathrooms and square footage. Public transport links consisting of the 3 closest forms of rail transport separated by type for example national rail and trams. Estate agent characteristics such as name, location and whether they are a developer. Council tax and other charges, such as council tax band and annual service charges. Listing characteristics such as how long the listing was a featured or premium listing. Transaction characteristics such as whether the property is up for auction, is part of the affordable homes scheme or if the property is a retirement property. Derived characteristics such as the length of the property's description, number of key features mentioned in the listing, the number of images in the listing, the average resolution of the property's images and text classification variables taken from the key features such as whether the property is described as coastal, rural, etc. Property type characteristics, these include physical characteristics such as detached or bungalow as well as whether the property is a residential or commercial listing.}

\textnormal{To ensure fair comparison between properties a number of filters and splits are applied to the properties. For example our analysis focuses on the residential market and from within that excludes and student properties as well as retirement properties, shared ownership properties and homes that are part of the affordable homes scheme.
This data contains some critical variables for analysis, the estate agents are provided a unique identifier and are explicitly linked to all properties that they have listed allowing for a guaranteed link between each property and their respective listing agent. We also are able to follow any price changes applied to the listing as the listing ages and the original listing date can be obtained as well as the date of the most recent price reduction if applied before the scraping process began.}

\subsection{Natural language processing}
\subsubsection{What is being processed}
\textnormal{The previous section highlights our use of text classification variables that are derived from the property's listing but are not inherently included in the listing themselves. The vast majority of property listings in our dataset make use of key features, these are short 1-2 sentences that outline some aspect of the property that the estate agent/lister deemed important enough to be included outside of the main description body. These include things like: "A beautiful enclosed garden", "Within close proximity to local amenities" and "Billiard room with Eastern aspect". This information is likely important for explaining behaviour of buyers, as it can help analyse preferences of buyers as well as analysing estate agent performance as this section is highly specific to each agent and allows them to highlight what they think are the most attractive parts of the property.
This information however is locked in natural language which can be very difficult to isolate semantic meaning. Simple keyword matching alone can lead to very inaccurate classifications due to some words having multiple meanings for example: "Access to local car park" and "Access to local play park" could be easily confused if we simply categorise any key feature with the word park as being either a park or a park. In both cases we are misclassifying one definition of the word park.}

\subsubsection{Pre-processing}
\textnormal{Text cannot be inherently interpreted by a computer and as such requires some preprocessing into a usable format. One of our methodologies requires the use of the cl100k\_base tokenizer which can be produced using the python library tiktoken. For the sake of consistency (so both methodologies see the same text) this tokenization process is used for both methodologies. This comes with the added benefit of cl100k\_base using sub word tokens which enhances the matching capabilities of BM25 (source, I don't have one but I have shown it anecdotally there's definitely a source out there).}

\subsubsection{Processing}
\textnormal{We make use of best matching 25 (BM25) which is a method that builds on top of term frequency-inverse document frequency. Effectively this uses the frequency of terms in documents in order to ascertain whether two texts are similar. For example: "play park" and "play ground" will may be seen as being related as play makes up a large amount of each one of them. However, this has the draw back of requiring some exact term matching so things like "yard", "garden" and "grounds" won't be matched directly but may be matched by context words which can be inaccurate. The BM25 model takes in a corpus of texts and then scores a provided text based on its similarity to any individual text in the corpus, this score is unbounded. 
This is where making use of more modern techniques like text embeddings provide extra context, these embeddings focus more on the semantic feel of words and are based on weightings provided by large pre-trained models. We make use of OpenAI's text-embedding-3-large model which has shown to be incredibly accurate when defining the semantic meaning of text. The model provides a matrix of 3072 scores between -1 and 1 that identify the semantic meaning of the text, these matrices can be used to find the semantic similarity between two different texts. This is done using the cosine similarity between the two texts.
We chose to process the texts by manually classifying 10,000 key features, some key features would have multiple classifications for example "A wonderful bedroom with an East facing balcony" would have 4 categories: beautiful (a catch all for generic flowery language), bedroom, East facing and balcony. If a category appeared in more than 1\% of the key features that were classified were then assigned a representative key feature that outlined only that key feature, where this was not possible this information was extracted from a key feature that had that classification and the fewest other classifications.}

\subsubsection{Classification}
\textnormal{These representative key features were used to generate scores using both the BM25 model and the text embedding cosine similarity methodologies. These scores are then both combined into a single score, this has been shown to significantly increase accuracy (https://www.anthropic.com/news/contextual-retrieval). This improvement can be thought of as BM25 emphasising matches where the base case has some word overlap with the corpus text while the cosine similarity will handle semantic similarity.
This combination is done by standardising both values to a score from 0 to 1. Since, we know beforehand (since the base case is derived from the corpus data) we know that each classification will have a perfect/best match and there will be no case where there should be no matches the guarantee of there being a score of 1 for each classification is of no consequence (I hope that's the case, please say it is). These scores are then combined based on a weighting, in this case the best performance was found at a weighting of 70\% cosine similarity and 30\% BM25 (I have no metric for this, just trust me). 
This provided a matrix of scores for each key feature highlighting how well it matched each classification. The only main issue here is there was no significant discontinuity toward the tails of the similarity scores. This was solved by finding the point at which classification became less than 90\% accurate, found by splitting the data into buckets based on score and analysing the results manually. This does mean that some data will not be perfectly classified, but that will always be the case even with manually labelled data, fortunately however the drop off in accuracy was sever at the 90\% mark very quickly decaying to 0\% in later bins (as expected for an accurate scoring system). 
At this point all elements with scores above the minimum in the bucket had their score adjusted (I need to find the exact formula I used) and all those below the minimum score of the bucket had their scores reduced in order to create a non-linear step function at the cut off point. 
This approach allows for a measure of the degree of mentioned that something was, for example a multi classified key feature such as "Kitchen, garden, bathroom and balcony with views of car park" will have a lower score for kitchen than "Fitted kitchen" would and a listing that mentions the same feature in multiple key features such as "Under floor heating in bathroom" and "Under floor heating in living room" will have a higher score for under floor heating than if the feature was only mentioned in one key feature. Thus giving an idea how how much the agent keyed in on any specific feature.}